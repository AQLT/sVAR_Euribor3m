---
title: "sVAR"
output: pdf_document
urlcolor: blue
header-includes:
- \usepackage{dsfont}
- \renewcommand{\P}{\mathds{P}}
- \newcommand\N{\mathds{N}}
- \newcommand\R{\mathds{R}}
- \newcommand\1{\mathds{1}}
- \newcommand\E{\mathds{E}}
- \newcommand\V{\mathds{V}}
- \newcommand\ud{\,\mathrm{d}}
- \DeclareMathOperator*{\argmax}{argmax}
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator{\e}{e}
- \DeclareMathOperator{\Cov}{Cov}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Prenons un modèle VAR(p) général avec $n$ composantes :
\[
X_{t}=\mu+\sum_{i=1}^{p}\Phi_{i}X_{t-i}+\varepsilon_{t},\quad\text{avec }\V\varepsilon_{t}=\Sigma
\]

On se place dans le cas d'un VAR stationaire donc tous les chocs sont
transitoires : l'effet d'un choc tend vers 0.

Dans le cas de l'étude des IRF, on va s'intéresser à l'impact d'un
choc unitaire sur une des composantes.

En théorie, pour calculer l'impact en $t+h$ on calcule la décomposition
de Wold :
\[
X_{t}=m+\sum_{k=0}^{\infty}C_{k}\varepsilon_{t-k}
\]

Et donc, si $C_{h}=\left[C_{1}^{(h)},\dots,C_{n}^{(h)}\right]=\left(c_{i,j}^{(h)}\right)_{1\leq i,j\leq n}$
alors : 
\[
\frac{\partial X_{t+h}}{\partial\varepsilon_{jt}}=C_{j}^{(h)}\text{ et }\frac{\partial x_{i,t+h}}{\partial\varepsilon_{jt}}=c_{i,j}^{(h)}
\]

Les IRF sont les graphes $h\mapsto c_{i,j}^{(h)}$.

En pratique en fait plutôt une simulation du VAR :
\[
\begin{cases}
X_{t-1}=\dots=X_{t-p}=0\\
X_{t}=\varepsilon_{t}=(0,\dots,0,1,0,\dots,0)'\\
\varepsilon_{t+s}=0\quad\forall s>0
\end{cases}
\]

Cette approche n'a du sens seulement si $\Sigma$ est diagonale :
dans le cas contraire, $\varepsilon_{jt}$ donne de l'information
sur les autres composantes de $\varepsilon_{t}$. C'est pourquoi on
utilise générale des IRF orthogonalisés.

L'idée est la suivante : à partir des $\varepsilon_{t}$, on construit
un vecteur $u_{t}$ qui contient la même information que $\varepsilon_{t}$
mais dont les composantes sont non corrélées, ce qui permet de considérer
des chocs unitaires sur les $u_{it}$.

La décomposition de Cholesky est la suivante :
\begin{itemize}
\item $u_{1t}=\varepsilon_{1t}$ et $\forall j>1,u_{jt}=\varepsilon_{jt}-\sum_{k=1}^{j-1}a_{jk}u_{kt}$
avec $a_{jk}$ choisis de sorte que $\Cov(u_{jt},u_{it})=0\forall i<j$.
\item $A$ est triangulaire inférieure avec des 1 sur la diagonale : $u_{t}=A^{-1}\varepsilon_{t}$
et $\V u_{t}=D$ diagonale. On a donc $\Sigma=ADA'=PP'$ avec $P=AD^{1/2}$
triangulaire inférieure.
\item En notant $A=\left(a_{1},\dots,a_{n}\right)$ il vient :
\begin{itemize}
\item L'effet d'un choc unitaire de $u_{jt}$ sur $\varepsilon_{it}$ est
$a_{ij}$ et l'effet d'un choc sur $X_{t+h}$ est $C_{h}a_{j}$. Un
choc sur $u_{jt}$ s'intérprète comme un choc sur $x_{jt}$ qui n'est
pas lié à des chocs sur $x_{1,t},\dots,x_{j-1,t}$.
\item Les IRF orthogonalisés sont les graphs $h\mapsto C_{h}a_{j}$ (avec
$a_{j}$ éventuellement normalisés de sorte que l'on considère la
variance du choc égale à un).
\end{itemize}
\end{itemize}

Dans un modèle SVAR(p), la spécification est légèrement différente : on prend un modèle du type
\[
AX_{t}=\mu+\sum_{i=1}^{p}\Phi_{i}AX_{t-i}+Bu_{t},
\]

On a donc $\varepsilon_{t}=A^{-1}Bu_{t}$ et les matrices $A$ et
$B$ peuvent être utilisées pour donner un sens économique aux chocs.
Dans l'exemple précédent, on a par exemple $A=\begin{pmatrix}* & 0 & 0 & 0\\
* & * & 0 & 0\\
* & * & * & 0\\
* & * & * & *
\end{pmatrix}$ et $B=I_{n}$, cela signifie qu'un choc sur la première composante
a un effet \textbf{contemporain} sur toutes les autres composantes
(1ere colonne). Un choc sur la deuxième variable n'a pas d'impact
\textbf{contemporain} sur la première variable mais en a sur toutes les autres
(un 0 sur la première ligne de la deuxième colonne), etc.

Sous R on spécifie $A$, $B$ ou les deux :
\begin{itemize}
\item Si l'on ne spécifie qu'une des deux matrices il faut au minimum $K(K-1)/2$
contraintes ($K=n=$ nombre de variables)
\item Si les deux sont spécifiées, il en faut au moins $K^{2}+K(K-1)/2$
\end{itemize}
Dans les exemples trouvés, généralement on ne spécifie qu'une des
deux matrices ou alors on normalise à 1 la diagonale de $A$, par
exemple pour la décomposition de Cholesky :

\[
A=\begin{pmatrix}1 & 0 & 0 & 0\\
* & 1 & 0 & 0\\
* & * & 1 & 0\\
* & * & * & 1
\end{pmatrix},\quad B=\begin{pmatrix}* & 0 & 0 & 0\\
0 & * & 0 & 0\\
0 & 0 & * & 0\\
0 & 0 & 0 & *
\end{pmatrix}
\]

Pour coder ça il suffit de remplacer les $*$ par des `NA` :
```{r}
Amat <- diag(nrow = 4)
Amat[2,1] <- Amat[3,1:2] <- Amat[4,1:3] <- NA
Amat
Bmat <- diag(nrow = 4)
diag(Bmat) <- NA
Bmat
```
Les contraintes que l'on peut imposer sont assez libres. [Ici](http://rstudio-pubs-static.s3.amazonaws.com/11201_92f20d52223d4028b98e9a4dae39c29c.html) par exemple, ils supposent que le PIB n'est affecté de manière contemporaine que par lui-même (première colonne de la matrice) et que les prix ne sont affectés que par le PIB et lui-même.

Dans le package `vars` il y a également la fonction `BQ` qui permet d'estimer un modèle SVAR de la même façon que dans l'article de Blanchard Quah (1989) vu en cours (mais je n'ai pas bien en tête ce que c'est)

